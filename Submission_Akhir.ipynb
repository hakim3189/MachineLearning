{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hakim3189/MachineLearning/blob/main/Submission_Akhir.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9wADwK78DCz"
      },
      "source": [
        "# Proyek Klasifikasi Gambar: [Input Nama Dataset]\n",
        "- **Nama:** Muhammad Rheza Iwanul Hakim\n",
        "- **Email:** rheza.hakim.89@gmail.com\n",
        "- **ID Dicoding:** rheza89"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-z4QGlO8DC1"
      },
      "source": [
        "## Import Semua Packages/Library yang Digunakan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FVYwaObI8DC1"
      },
      "outputs": [],
      "source": [
        "# Most Used Library\n",
        "import os, shutil\n",
        "import zipfile\n",
        "import random\n",
        "from random import sample\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm as tq\n",
        "# Libraries for Image Processing\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import skimage\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "from skimage.transform import rotate, AffineTransform, warp\n",
        "from skimage import img_as_ubyte\n",
        "from skimage.exposure import adjust_gamma\n",
        "from skimage.util import random_noise\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # This line imports the seaborn library and assigns it to the alias 'sns'\n",
        "# Libraries for Build Model\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, layers\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.layers import InputLayer, Conv2D, SeparableConv2D, MaxPooling2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "metadata": {
        "id": "QKcd_bFh3T18"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "TK4DvqfbYrN8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHekw29KX4XQ"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wIcv1F9fX4XQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "b6ec8903-6a14-4b4e-e04f-b4d07a6465ac"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-df71cef1-4aa4-4001-a641-0ad65d6f16d6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-df71cef1-4aa4-4001-a641-0ad65d6f16d6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle (1).json': b'{\"username\":\"rhezahakim\",\"key\":\"7d59da0af5d9999d1abcb09deec1950d\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d andrewmvd/animal-faces\n",
        "!unzip animal-faces.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj2ML57w3kox",
        "outputId": "1533839f-d37b-4ef1-f8d8-070bc7e77828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/andrewmvd/animal-faces\n",
            "License(s): Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)\n",
            "animal-faces.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  animal-faces.zip\n",
            "replace afhq/train/cat/flickr_cat_000002.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil"
      ],
      "metadata": {
        "id": "ZghO-Ute34zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Direktori awal untuk train dan test\n",
        "train_dir = \"afhq/train\"\n",
        "test_dir = \"afhq/val\"\n",
        "\n",
        "# Direktori baru untuk dataset gabungan\n",
        "combined_dir = \"afhq/dataset\""
      ],
      "metadata": {
        "id": "F0I5KS1B37tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat direktori baru untuk dataset gabungan\n",
        "os.makedirs(combined_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "uFVEjucm4FFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salin file dan folder dari train\n",
        "for category in os.listdir(train_dir):\n",
        "    category_dir = os.path.join(train_dir, category)\n",
        "    if os.path.isdir(category_dir):\n",
        "        shutil.copytree(category_dir, os.path.join(combined_dir, category), dirs_exist_ok=True)\n",
        "\n",
        "# Salin file dan folder dari test\n",
        "for category in os.listdir(test_dir):\n",
        "    category_dir = os.path.join(test_dir, category)\n",
        "    if os.path.isdir(category_dir):\n",
        "        shutil.copytree(category_dir, os.path.join(combined_dir, category), dirs_exist_ok=True)"
      ],
      "metadata": {
        "id": "wE_hemrS4M6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat kamus yang menyimpan gambar untuk setiap kelas dalam data\n",
        "animal_faces_image = {}\n",
        "\n",
        "# Tentukan path sumber train\n",
        "path = \"afhq/\"\n",
        "path_sub = os.path.join(path, \"dataset\")\n",
        "for i in os.listdir(path_sub):\n",
        "    animal_faces_image[i] = os.listdir(os.path.join(path_sub, i))\n",
        "\n",
        "# Menampilkan secara acak 5 gambar di bawah setiap dari 2 kelas dari data.\n",
        "# Anda akan melihat gambar yang berbeda setiap kali kode ini dijalankan.\n",
        "path_sub = \"afhq/dataset/\"\n",
        "\n",
        "# Menampilkan secara acak 5 gambar di bawah setiap kelas dari data latih\n",
        "fig, axs = plt.subplots(len(animal_faces_image.keys()), 5, figsize=(15, 15))\n",
        "\n",
        "for i, class_name in enumerate(os.listdir(path_sub)):\n",
        "    images = np.random.choice(animal_faces_image[class_name], 5, replace=False)\n",
        "\n",
        "    for j, image_name in enumerate(images):\n",
        "        img_path = os.path.join(path_sub, class_name, image_name)\n",
        "        img = Image.open(img_path)\n",
        "        axs[i, j].imshow(img, cmap='gray')\n",
        "        axs[i, j].set(xlabel=class_name, xticks=[], yticks=[])\n",
        "\n",
        "\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "G5OQJBRb5D_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisikan path sumber\n",
        "lung_path = \"afhq/dataset/\"\n",
        "\n",
        "# Buat daftar yang menyimpan data untuk setiap nama file, path file, dan label dalam data\n",
        "file_name = []\n",
        "labels = []\n",
        "full_path = []\n",
        "\n",
        "# Dapatkan nama file gambar, path file, dan label satu per satu dengan looping, dan simpan sebagai dataframe\n",
        "for path, subdirs, files in os.walk(lung_path):\n",
        "    for name in files:\n",
        "        full_path.append(os.path.join(path, name))\n",
        "        labels.append(path.split('/')[-1])\n",
        "        file_name.append(name)\n",
        "\n",
        "distribution_train = pd.DataFrame({\"path\":full_path, 'file_name':file_name, \"labels\":labels})\n",
        "\n",
        "# Plot distribusi gambar di setiap kelas\n",
        "Label = distribution_train['labels']\n",
        "plt.figure(figsize = (6,6))\n",
        "sns.set_style(\"darkgrid\")\n",
        "plot_data = sns.countplot(Label)"
      ],
      "metadata": {
        "id": "7P4F_uSM6JEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFgLyQPHX98s"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split Dataset"
      ],
      "metadata": {
        "id": "9ICO2-E0YxzD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HJokQbxX98s"
      },
      "outputs": [],
      "source": [
        "mypath= 'afhq/dataset/'\n",
        "\n",
        "file_name = []\n",
        "labels = []\n",
        "full_path = []\n",
        "for path, subdirs, files in os.walk(mypath):\n",
        "    for name in files:\n",
        "        full_path.append(os.path.join(path, name))\n",
        "        labels.append(path.split('/')[-1])\n",
        "        file_name.append(name)\n",
        "\n",
        "# Memasukkan variabel yang sudah dikumpulkan pada looping di atas menjadi sebuah dataframe agar rapi\n",
        "df = pd.DataFrame({\"path\":full_path,'file_name':file_name,\"labels\":labels})\n",
        "# Melihat jumlah data gambar pada masing-masing label\n",
        "df.groupby(['labels']).size()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variabel yang digunakan pada pemisahan data ini di mana variabel x = data path dan y = data labels\n",
        "\n",
        "X= df['path']\n",
        "y= df['labels']\n",
        "\n",
        "# Split dataset awal menjadi data train dan test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=300)"
      ],
      "metadata": {
        "id": "oQNwcCI26gNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyatukan ke dalam masing-masing dataframe\n",
        "df_tr = pd.DataFrame({'path':X_train,'labels':y_train,'set':'train'})\n",
        "df_te = pd.DataFrame({'path':X_test,'labels':y_test,'set':'test'})"
      ],
      "metadata": {
        "id": "zuIGwPPo6tkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gabungkan DataFrame df_tr dan df_te\n",
        "df_all = pd.concat([df_tr, df_te], ignore_index=True)\n",
        "\n",
        "print('===================================================== \\n')\n",
        "print(df_all.groupby(['set', 'labels']).size(), '\\n')\n",
        "print('===================================================== \\n')\n",
        "\n",
        "# Cek sampel data\n",
        "print(df_all.sample(5))\n",
        "\n",
        "# Memanggil dataset asli yang berisi keseluruhan data gambar yang sesuai dengan labelnya\n",
        "datasource_path = \"afhq/dataset/\"\n",
        "# Membuat variabel Dataset, tempat menampung data yang telah dilakukan pembagian data training dan testing\n",
        "dataset_path = \"Dataset-Final/\""
      ],
      "metadata": {
        "id": "5-SkVr30603o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in tq(df_all.iterrows()):\n",
        "    # Deteksi filepath\n",
        "    file_path = row['path']\n",
        "    if os.path.exists(file_path) == False:\n",
        "            file_path = os.path.join(datasource_path,row['labels'],row['image'].split('.')[0])\n",
        "\n",
        "    # Buat direktori tujuan folder\n",
        "    if os.path.exists(os.path.join(dataset_path,row['set'],row['labels'])) == False:\n",
        "        os.makedirs(os.path.join(dataset_path,row['set'],row['labels']))\n",
        "\n",
        "    # Tentukan tujuan file\n",
        "    destination_file_name = file_path.split('/')[-1]\n",
        "    file_dest = os.path.join(dataset_path,row['set'],row['labels'],destination_file_name)\n",
        "\n",
        "    # Salin file dari sumber ke tujuan\n",
        "    if os.path.exists(file_dest) == False:\n",
        "        shutil.copy2(file_path,file_dest)"
      ],
      "metadata": {
        "id": "LHMjmikW7DIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DIR = \"Dataset-Final/train\"\n",
        "TEST_DIR = \"Dataset-Final/test\"\n",
        "\n",
        "train_cat = os.path.join(TRAIN_DIR + '/cat')\n",
        "train_dog = os.path.join(TRAIN_DIR + '/dog')\n",
        "train_wild = os.path.join(TRAIN_DIR + '/wild')\n",
        "test_cat = os.path.join(TEST_DIR + '/cat')\n",
        "test_dog = os.path.join(TEST_DIR + '/dog')\n",
        "test_wild = os.path.join(TEST_DIR + '/wild')\n",
        "\n",
        "print(\"Total number of cat images in training set: \",len(os.listdir(train_cat)))\n",
        "print(\"Total number of dog images in training set: \",len(os.listdir(train_dog)))\n",
        "print(\"Total number of wild images in training set: \",len(os.listdir(train_wild)))\n",
        "print(\"Total number of cat images in test set: \",len(os.listdir(test_cat)))\n",
        "print(\"Total number of dog images in test set: \",len(os.listdir(test_dog)))\n",
        "print(\"Total number of wild images in test set: \",len(os.listdir(test_wild)))"
      ],
      "metadata": {
        "id": "DFwU3q-r7ZrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat objek ImageDataGenerator yang menormalkan gambar\n",
        "datagen = ImageDataGenerator(rescale=1/255.,\n",
        "                             validation_split = 0.2,\n",
        "                             rotation_range=20,  # Rotate images by up to 20 degrees\n",
        "                             width_shift_range=0.2,  # Shift images horizontally by up to 20% of the width\n",
        "                             height_shift_range=0.2,  # Shift images vertically by up to 20% of the height\n",
        "                             shear_range=0.2,  # Apply shear transformations\n",
        "                             zoom_range=0.2,  # Zoom in or out on images\n",
        "                             horizontal_flip=True  # Flip images horizontally\n",
        "                             )\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(TRAIN_DIR,\n",
        "                                              batch_size=32,\n",
        "                                              target_size=(150,150),\n",
        "                                              color_mode=\"grayscale\",\n",
        "                                              class_mode='categorical',\n",
        "                                              subset='training',\n",
        "                                              shuffle=True)\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(TRAIN_DIR,\n",
        "                                                   batch_size=32,\n",
        "                                                   target_size=(150,150),\n",
        "                                                color_mode=\"grayscale\",\n",
        "                                                   class_mode='categorical',\n",
        "                                                   subset='validation',\n",
        "                                                   shuffle=False)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(TEST_DIR,\n",
        "                                                  batch_size=1,\n",
        "                                                  target_size=(150,150),\n",
        "                                                  color_mode=\"grayscale\",\n",
        "                                                  class_mode='categorical',\n",
        "                                                  shuffle=False)"
      ],
      "metadata": {
        "id": "lcMQCGAP8vNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc-Ph-oIYAUU"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTwK0t8XYAUU"
      },
      "outputs": [],
      "source": [
        "# tf.keras.backend.clear_session()\n",
        "\n",
        "####################### Init sequential model ##################################\n",
        "model_1 = Sequential()\n",
        "\n",
        "# ######################### Input layer with Fully Connected Layer ################################\n",
        "# 1st Convolutional layer, Batch Normalization layer, and Pooling layer\n",
        "model_1.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(150,150,1)))\n",
        "model_1.add(BatchNormalization())\n",
        "model_1.add(MaxPool2D((2, 2)))\n",
        "\n",
        "# 2nd Convolutional layer, Batch Normalization layer, and Pooling layer\n",
        "model_1.add(Conv2D(32, (4, 4),padding='same', activation='relu'))\n",
        "model_1.add(BatchNormalization())\n",
        "model_1.add(MaxPool2D((2, 2)))\n",
        "\n",
        "# 3rd Convolutional layer, Batch Normalization layer, and Pooling layer\n",
        "model_1.add(Conv2D(32, (7, 7), padding='same', activation='relu'))\n",
        "model_1.add(BatchNormalization())\n",
        "model_1.add(MaxPool2D((2, 2)))\n",
        "\n",
        "# Flatten layer\n",
        "model_1.add(Flatten())\n",
        "# 1nd Dense Layer\n",
        "model_1.add(Dense(128, activation = 'relu'))\n",
        "# 1nd Dropout Layer\n",
        "model_1.add(Dropout(0.5))\n",
        "# 2nd Dense Layer\n",
        "model_1.add(Dense(64, activation = 'relu'))\n",
        "# 2nd Dropout Layer\n",
        "model_1.add(Dropout(0.4))\n",
        "\n",
        "# Final Dense layer\n",
        "model_1.add(Dense(3, activation='softmax'))\n",
        "######################### Fully Connected Layer ################################\n",
        "\n",
        "######################### Compile Model ################################\n",
        "model_1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the Model Architecture\n",
        "print(model_1.summary())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "ADOsYsBdzNIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_cat = os.path.join(TRAIN_DIR + '/cat')\n",
        "train_dog = os.path.join(TRAIN_DIR + '/dog')\n",
        "train_wild = os.path.join(TRAIN_DIR + '/wild')\n",
        "\n",
        "# Calculate class weights for three classes\n",
        "count_cat, count_dog, count_wild = len(os.listdir(train_cat)), len(os.listdir(train_dog)), len(os.listdir(train_wild))\n",
        "total_samples = count_cat + count_dog + count_wild\n",
        "\n",
        "weight_cat = (1 / count_cat) * (total_samples) / 3.0\n",
        "weight_dog = (1 / count_dog) * (total_samples) / 3.0\n",
        "weight_wild = (1 / count_wild) * (total_samples) / 3.0\n",
        "\n",
        "class_weights = {0: weight_cat, 1: weight_dog, 2: weight_wild}  # Assuming 0:cat, 1:dog, 2:wild\n",
        "\n",
        "# Fitting / training model\n",
        "history_1 = model_1.fit(train_generator,\n",
        "                        epochs=30,\n",
        "                        batch_size=32,\n",
        "                        validation_data=validation_generator,\n",
        "                        class_weight = class_weights,\n",
        "                        callbacks=[checkpoint, early_stopping])"
      ],
      "metadata": {
        "id": "fC0LpsVm8-CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XctzCfzbYCBK"
      },
      "source": [
        "## Evaluasi dan Visualisasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKk-ScZWYCBK"
      },
      "outputs": [],
      "source": [
        "acc = history_1.history['accuracy']\n",
        "val_acc = history_1.history['val_accuracy']\n",
        "loss = history_1.history['loss']\n",
        "val_loss = history_1.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r')\n",
        "plt.plot(epochs, val_acc, 'b')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, loss, 'r')\n",
        "plt.plot(epochs, val_loss, 'b')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_fIsUogYFSk"
      },
      "source": [
        "## Konversi Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZvGBpYoYFSl"
      },
      "outputs": [],
      "source": [
        "model_1.save(\"model.h5\")\n",
        "# Install tensorflowjs\n",
        "!pip install tensorflowjs\n",
        "\n",
        "# Convert model.h5 to model\n",
        "!tensorflowjs_converter --input_format=keras model.h5 tfjs_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "export_dir = 'saved_model/'\n",
        "tf.saved_model.save(model_1, export_dir)\n",
        "\n",
        "# Convert SavedModel menjadi vegs.tflite\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_model_file = pathlib.Path('vegs.tflite')\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "metadata": {
        "id": "RfKOuLs1uyWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference (Optional)"
      ],
      "metadata": {
        "id": "8DbfEwvvm5U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from PIL import Image\n",
        "\n",
        "# Load the trained model\n",
        "model = tf.keras.models.load_model('model.h5')\n",
        "\n",
        "def predict_image(image_path):\n",
        "    img = image.load_img(image_path, target_size=(150, 150), color_mode=\"grayscale\")\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array /= 255.\n",
        "\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_class = np.argmax(prediction)\n",
        "\n",
        "    class_labels = {0: 'cat', 1: 'dog', 2: 'wild'} # Replace with your actual class labels\n",
        "    predicted_label = class_labels.get(predicted_class, \"Unknown\")\n",
        "\n",
        "    return predicted_label, prediction\n",
        "\n",
        "\n",
        "# Example usage\n",
        "image_path = \"New_Cat_Data.jpg\" # Replace with the path to your image\n",
        "predicted_label, prediction = predict_image(image_path)\n",
        "print(f\"Predicted label: {predicted_label}\")\n",
        "print(f\"Prediction probabilities: {prediction}\")\n"
      ],
      "metadata": {
        "id": "ue5esMSSm8GQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}